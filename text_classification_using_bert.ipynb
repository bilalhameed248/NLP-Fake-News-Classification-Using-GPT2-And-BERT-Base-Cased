{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3e7ec4",
   "metadata": {},
   "source": [
    "## Installing  Librries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2451ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "# pip install datasets\n",
    "# pip install ipywidgets\n",
    "# pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a3db7",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e1a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime \n",
    "import pandas as pd \n",
    "from transformers import BertTokenizer, AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, pipeline\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee5411",
   "metadata": {},
   "source": [
    "## Get Current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec42a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir = os.getcwd() + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6f511",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ead9273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3ab95468b09c363b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Bilal\\.cache\\huggingface\\datasets\\csv\\default-3ab95468b09c363b\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d686e1fe2bcb47dd82a6a55ef93788fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4919250660430f85628eb9bedc1836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Bilal\\.cache\\huggingface\\datasets\\csv\\default-3ab95468b09c363b\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b59265320e4e64abfb48f4144468cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': './train_df.csv', 'test': './test_df.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d2de4",
   "metadata": {},
   "source": [
    "## Bert Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111c81a",
   "metadata": {},
   "source": [
    "<p>Here we load BERT tokenizer and saving it in directory folder save_tokenizer</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d7e4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e700d5e1fe6b4b60a979d9669ed305a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b2c8cbe51c4fbda21092e0cd4a4dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242ec60d6b1b46db84f7b986fb875d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b52c5f43a104b28b81c70d735f4e1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Bilal\\\\Desktop\\\\NLP\\\\Fiver\\\\bert/save_tokenizer\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\NLP\\\\Fiver\\\\bert/save_tokenizer\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\NLP\\\\Fiver\\\\bert/save_tokenizer\\\\vocab.txt',\n",
       " 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\NLP\\\\Fiver\\\\bert/save_tokenizer\\\\added_tokens.json',\n",
       " 'C:\\\\Users\\\\Bilal\\\\Desktop\\\\NLP\\\\Fiver\\\\bert/save_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.save_pretrained(my_dir+\"save_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e2c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./save_tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620a3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851d6af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd5bf372ba7463b88bc7c0cc4075e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cf67c2a9ba42988dc9f917e95c8d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "small_train_dataset = tokenized_datasets['train']\n",
    "small_eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2aed4c",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f210d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7e919",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d159bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(my_dir+\"save_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ddb9c",
   "metadata": {},
   "source": [
    "## Loading Modeel From Save Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5a3870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./save_model/\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684145c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f07e7a65c44b82bf8a0ac3b1ca4d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f54b1b",
   "metadata": {},
   "source": [
    "## Setting TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5f48b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "         overwrite_output_dir = 'True',\n",
    "         output_dir=\"./fined_tuned_model/\",\n",
    "         evaluation_strategy=\"epoch\",\n",
    "         num_train_epochs=3, \n",
    "         save_total_limit = 2,\n",
    "         save_strategy = \"no\",\n",
    "         load_best_model_at_end=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117ed1f",
   "metadata": {},
   "source": [
    "## Setting Trainer Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7863753",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset ,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb413352",
   "metadata": {},
   "source": [
    "## Training Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82c4e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Bilal\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2016\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 756\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 10:03:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192468</td>\n",
       "      <td>0.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.024874</td>\n",
       "      <td>0.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.998000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=756, training_loss=0.21630305461782626, metrics={'train_runtime': 36266.4035, 'train_samples_per_second': 0.167, 'train_steps_per_second': 0.021, 'total_flos': 1591295662817280.0, 'train_loss': 0.21630305461782626, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f1c3b",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58101556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./fined_tuned_model/\n",
      "Configuration saved in ./fined_tuned_model/config.json\n",
      "Model weights saved in ./fined_tuned_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790a8bf",
   "metadata": {},
   "source": [
    "## Calling Prediction On Eval Datasat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "970df9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(small_eval_dataset)\n",
    "print(list(np.argmax(predictions.predictions, axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9500fd0",
   "metadata": {},
   "source": [
    "## Loading the save tokeniizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff94d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./save_tokenizer/tokenizer.json. We won't load it.\n",
      "Didn't find file ./save_tokenizer/added_tokens.json. We won't load it.\n",
      "loading file ./save_tokenizer/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file ./save_tokenizer/special_tokens_map.json\n",
      "loading file ./save_tokenizer/tokenizer_config.json\n",
      "loading configuration file ./fined_tuned_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./fined_tuned_model/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./fined_tuned_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./fined_tuned_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Label 0 = test1\n",
      "Label 1 = test2\n",
      "\n",
      "Enter Text\n",
      "We were greeted in an inquisitive but friendly manner and handed a coconut fresh from the tree . \n",
      "\n",
      "Sentence: We were greeted in an inquisitive but friendly manner and handed a coconut fresh from the tree . \n",
      "\n",
      " [{'label': 'LABEL_0', 'score': 0.9998779296875}]\n",
      "Label:  LABEL_0\n",
      "Accuracy: 0.9998779296875\n",
      "Enter Text\n",
      "She was never taken out of the bed , until the ambulance came and took her out the bed this morning .\n",
      "\n",
      "Sentence: She was never taken out of the bed , until the ambulance came and took her out the bed this morning .\n",
      "\n",
      " [{'label': 'LABEL_1', 'score': 0.9997208714485168}]\n",
      "Label:  LABEL_1\n",
      "Accuracy: 0.9997208714485168\n",
      "Enter Text\n",
      "Not long after takeoff from Madrid , the airplane discovers a problem with the landing gear .\n",
      "\n",
      "Sentence: Not long after takeoff from Madrid , the airplane discovers a problem with the landing gear .\n",
      "\n",
      " [{'label': 'LABEL_1', 'score': 0.9997456669807434}]\n",
      "Label:  LABEL_1\n",
      "Accuracy: 0.9997456669807434\n",
      "Enter Text\n",
      "Do Not Skip the doctor\n",
      "\n",
      "Sentence: Do Not Skip the doctor\n",
      "\n",
      " [{'label': 'LABEL_0', 'score': 0.9351155161857605}]\n",
      "Label:  LABEL_0\n",
      "Accuracy: 0.9351155161857605\n",
      "Enter Text\n",
      "We did some demos and we asked Tony to come along and play keyboard .\n",
      "\n",
      "Sentence: We did some demos and we asked Tony to come along and play keyboard .\n",
      "\n",
      " [{'label': 'LABEL_0', 'score': 0.999451220035553}]\n",
      "Label:  LABEL_0\n",
      "Accuracy: 0.999451220035553\n",
      "Enter Text\n",
      "Do not skip the doctor \n",
      "\n",
      "Sentence: Do not skip the doctor \n",
      "\n",
      " [{'label': 'LABEL_1', 'score': 0.9998555183410645}]\n",
      "Label:  LABEL_1\n",
      "Accuracy: 0.9998555183410645\n",
      "Enter Text\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./save_tokenizer/')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./fined_tuned_model/\", num_labels=2)\n",
    "#calling prediction\n",
    "\n",
    "print(\"\\n\\nLabel 0 = test1\\nLabel 1 = test2\\n\")\n",
    "for i in range(20):\n",
    "    print(\"Enter Text\")\n",
    "    text=str(input())\n",
    "    print(\"\\nSentence: {}\".format(text))\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    result=classifier(text)\n",
    "    print(\"\\n\",result)\n",
    "    label=result[0]['label']\n",
    "    accuracy=result[0]['score']\n",
    "    print(\"Label: \",label)\n",
    "    print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c1df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a7eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
